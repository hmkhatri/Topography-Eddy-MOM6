{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\text{The notebook is for postprocessing of shallow water model simulations run with different wind stress profiles.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import dask\n",
    "import dask.distributed\n",
    "from dask.diagnostics import ProgressBar\n",
    "from xgcm import Grid\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "\n",
    "#cluster = LocalCluster()\n",
    "#client = Client(cluster)\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://140.208.147.173:32996' processes=0 threads=0, memory=0 B>\n",
      "http://localhost:30520/status\n"
     ]
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "portdash = 30520\n",
    "cluster = SLURMCluster(queue='analysis', cores=4, project='gfdl_o',memory=\"24GB\", \n",
    "                       walltime=\"03:00:00\",dashboard_address=f':{portdash}')\n",
    "cluster.scale(2)\n",
    "client = Client(cluster)\n",
    "print(client)\n",
    "print('http://localhost:'+str(portdash)+'/status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using 10-day averaged fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prog.nc files for all runs (for 40-50 years simulation time. The simulations are in equilibrium state)\n",
    "tau = ['Tau_001', 'Tau_003', 'Tau_005', 'Tau_01', 'Tau_03', 'Tau_05', 'Tau_1']\n",
    "ppdir = \"/work/Hemant.Khatri/MOM6_idealised/SWM_Two_Layer/Barotropic_Baroclinic/\"\n",
    "\n",
    "dataset_FW = []\n",
    "dataset_VW = []\n",
    "\n",
    "case = 'fplane'  # valid options are betaplane, fplane\n",
    "\n",
    "t_st = 180 # starting time for averaging (use t_st = 180 for fplane as these runs used restart files from betaplane)\n",
    "\n",
    "for i in range(0, len(tau)):\n",
    "    if (case == 'betaplane'):\n",
    "        file1 = ppdir+\"2L_Fix_Width/\"+tau[i]+\"/prog_avg_50.nc\"\n",
    "        file2 = ppdir+\"2L_Var_Width/\"+tau[i]+\"/prog_avg_50.nc\"\n",
    "    elif (case == 'fplane'):\n",
    "        file1 = ppdir+\"2L_fplane_Fix_Width/\"+tau[i]+\"/prog_avg_50.nc\"\n",
    "        file2 = ppdir+\"2L_fplane_Var_Width/\"+tau[i]+\"/prog_avg_50.nc\"\n",
    "    else:\n",
    "        sys.exit(\"case selection no valid\")\n",
    "        \n",
    "    d = xr.open_dataset(file1, chunks = {'Time': 1}, decode_times=False) # chunks required\n",
    "    dataset_FW.append(d)\n",
    "    d = xr.open_dataset(file2, chunks = {'Time': 1}, decode_times=False) # for parallel compu.\n",
    "    dataset_VW.append(d)\n",
    "    \n",
    "ds_FW = xr.concat(dataset_FW, dim='tau')\n",
    "ds_VW = xr.concat(dataset_VW, dim='tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read topography and wind stress profile data\n",
    "ta = ['1.0', '3.0', '5.0', '10.0', '30.0', '50.0', '100.0']\n",
    "\n",
    "ppdir = \"/nbhome/Hemant.Khatri/Topography-Eddy-MOM6/input-files/\"\n",
    "\n",
    "dataset_FW = []\n",
    "dataset_VW = []\n",
    "\n",
    "for i in range(0, len(ta)):\n",
    "    d = xr.open_dataset(ppdir+\"wind_stress_FW_tau_\" + ta[i] + \".nc\")\n",
    "    dataset_FW.append(d)\n",
    "    d = xr.open_dataset(ppdir+\"wind_stress_VW_tau_\" + ta[i] + \".nc\")\n",
    "    dataset_VW.append(d)\n",
    "\n",
    "wind_FW = xr.concat(dataset_FW, dim='tau')\n",
    "wind_VW = xr.concat(dataset_VW, dim='tau')\n",
    "\n",
    "topo = xr.open_dataset(ppdir+\"topography_2L.nc\")\n",
    "                       \n",
    "ds_FW['taux'] = xr.DataArray(wind_FW['taux'].values,dims=['tau','yh','xh'])\n",
    "ds_VW['taux'] = xr.DataArray(wind_VW['taux'].values,dims=['tau','yh','xh'])\n",
    "\n",
    "ds_FW['depth'] = xr.DataArray(topo['depth'].values,dims=['yh','xh'])\n",
    "ds_VW['depth'] = xr.DataArray(topo['depth'].values,dims=['yh','xh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select time range\n",
    "ds_FW = ds_FW.isel(Time = slice(t_st, len(ds_FW['Time']) -1))\n",
    "ds_VW = ds_VW.isel(Time = slice(t_st, len(ds_VW['Time']) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:     (Time: 184, nv: 2, tau: 7, xh: 400, xq: 400, yh: 800, yq: 800, zl: 2)\n",
       "Coordinates:\n",
       "  * xh          (xh) float64 2.0 6.0 10.0 14.0 ... 1.59e+03 1.594e+03 1.598e+03\n",
       "  * yh          (yh) float64 -1.598e+03 -1.594e+03 ... 1.594e+03 1.598e+03\n",
       "  * yq          (yq) float64 -1.596e+03 -1.592e+03 ... 1.596e+03 1.6e+03\n",
       "  * Time        (Time) float64 1.640e+04 1.642e+04 ... 1.822e+04 1.824e+04\n",
       "  * nv          (nv) float64 1.0 2.0\n",
       "  * xq          (xq) float64 4.0 8.0 12.0 16.0 ... 1.592e+03 1.596e+03 1.6e+03\n",
       "  * zl          (zl) float64 1.027e+03 1.028e+03\n",
       "Dimensions without coordinates: tau\n",
       "Data variables:\n",
       "    u           (tau, Time, zl, yh, xq) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    v           (tau, Time, zl, yq, xh) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    h           (tau, Time, zl, yh, xh) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    uh          (tau, Time, zl, yh, xq) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    vh          (tau, Time, zl, yq, xh) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    taux_bot    (tau, Time, yh, xq) float32 dask.array<chunksize=(1, 1, 800, 400), meta=np.ndarray>\n",
       "    tauy_bot    (tau, Time, yq, xh) float32 dask.array<chunksize=(1, 1, 800, 400), meta=np.ndarray>\n",
       "    average_T1  (tau, Time) float64 dask.array<chunksize=(1, 1), meta=np.ndarray>\n",
       "    average_T2  (tau, Time) float64 dask.array<chunksize=(1, 1), meta=np.ndarray>\n",
       "    average_DT  (tau, Time) float64 dask.array<chunksize=(1, 1), meta=np.ndarray>\n",
       "    Time_bnds   (tau, Time, nv) float64 dask.array<chunksize=(1, 1, 2), meta=np.ndarray>\n",
       "    taux        (tau, yh, xh) float64 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
       "    depth       (yh, xh) float64 4e+03 4e+03 4e+03 4e+03 ... 4e+03 4e+03 4e+03\n",
       "Attributes:\n",
       "    filename:   prog_avg.nc\n",
       "    title:      Channel\n",
       "    grid_type:  regular\n",
       "    grid_tile:  N/A"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_FW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\text{Computations}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid create for computations\n",
    "grid_FW = Grid(ds_FW, coords={'X': {'center': 'xh', 'right': 'xq'},\n",
    "                        'Y': {'center': 'yh', 'right': 'yq'}}, periodic=['X']);\n",
    "\n",
    "grid_VW = Grid(ds_VW, coords={'X': {'center': 'xh', 'right': 'xq'},\n",
    "                        'Y': {'center': 'yh', 'right': 'yq'}}, periodic=['X']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for computations\n",
    "\n",
    "def TW_mean_eddy(ds, grid):\n",
    "    \n",
    "    hm = ds['h'].mean('Time')\n",
    "    \n",
    "    hum = grid.interp(hm, 'X', boundary='fill') # interpolate h on u, v points\n",
    "    hvm = grid.interp(hm, 'Y', boundary='fill')\n",
    "    \n",
    "    um = ds['uh'].mean('Time') / hum # thickness-weighted mean velocities\n",
    "    vm = ds['vh'].mean('Time') / hvm\n",
    "    \n",
    "    ue = ds['u'] - um; ve = ds['v'] - vm # thickness-weighted eddy velocities\n",
    "    \n",
    "    rho = ds['zl'].mean()\n",
    "    \n",
    "    hu = grid.interp(ds['h'], 'X', boundary='fill')\n",
    "    hv = grid.interp(ds['h'], 'Y', boundary='fill')\n",
    "    \n",
    "    KE_m_zon = 0.5 * rho * (hum * um * um);  KE_m_mer = 0.5 * rho * (hvm * vm * vm)\n",
    "    KE_e_zon = 0.5 * rho * (hu * ue * ue).mean('Time');  KE_e_mer = 0.5 * rho * (hv * ve * ve).mean('Time')\n",
    "    \n",
    "    ds1 = xr.Dataset()\n",
    "    ds1['um'] = ds['u'].mean('Time'); ds1['vm'] = ds['v'].mean('Time')\n",
    "    ds1['uhm'] = ds['uh'].mean('Time'); ds1['vhm'] = ds['vh'].mean('Time')\n",
    "    ds1['hm'] = hm\n",
    "    \n",
    "    ds1['Zon_KE_mean'] = KE_m_zon; ds1['Mer_KE_mean'] = KE_m_mer; \n",
    "    ds1['Zon_KE_eddy'] = KE_e_zon; ds1['Mer_KE_eddy'] = KE_e_mer; \n",
    "    \n",
    "    ds1['taux_bot'] = ds['taux_bot'].mean('Time')\n",
    "    ds1['tauy_bot'] = ds['tauy_bot'].mean('Time')\n",
    "    \n",
    "    return ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pth = \"/work/Hemant.Khatri/MOM6_idealised/SWM_Two_Layer/Barotropic_Baroclinic/Postprocessing/\"\n",
    "FW = TW_mean_eddy(ds_FW, grid_FW)\n",
    "VW = TW_mean_eddy(ds_VW, grid_VW)\n",
    "\n",
    "FW['taux'] = ds_FW['taux']\n",
    "VW['taux'] = ds_VW['taux']\n",
    "FW['depth'] = ds_FW['depth']\n",
    "VW['depth'] = ds_VW['depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:      (tau: 7, xh: 400, xq: 400, yh: 800, yq: 800, zl: 2)\n",
       "Coordinates:\n",
       "  * yh           (yh) float64 -1.598e+03 -1.594e+03 ... 1.594e+03 1.598e+03\n",
       "  * xq           (xq) float64 4.0 8.0 12.0 16.0 ... 1.592e+03 1.596e+03 1.6e+03\n",
       "  * zl           (zl) float64 1.027e+03 1.028e+03\n",
       "  * xh           (xh) float64 2.0 6.0 10.0 14.0 ... 1.59e+03 1.594e+03 1.598e+03\n",
       "  * yq           (yq) float64 -1.596e+03 -1.592e+03 ... 1.596e+03 1.6e+03\n",
       "Dimensions without coordinates: tau\n",
       "Data variables:\n",
       "    um           (tau, zl, yh, xq) float32 dask.array<chunksize=(1, 2, 800, 400), meta=np.ndarray>\n",
       "    vm           (tau, zl, yq, xh) float32 dask.array<chunksize=(1, 2, 800, 400), meta=np.ndarray>\n",
       "    uhm          (tau, zl, yh, xq) float32 dask.array<chunksize=(1, 2, 800, 400), meta=np.ndarray>\n",
       "    vhm          (tau, zl, yq, xh) float32 dask.array<chunksize=(1, 2, 800, 400), meta=np.ndarray>\n",
       "    hm           (tau, zl, yh, xh) float32 dask.array<chunksize=(1, 2, 800, 400), meta=np.ndarray>\n",
       "    Zon_KE_mean  (tau, zl, yh, xq) float64 dask.array<chunksize=(1, 2, 800, 399), meta=np.ndarray>\n",
       "    Mer_KE_mean  (tau, zl, yq, xh) float64 dask.array<chunksize=(1, 2, 799, 400), meta=np.ndarray>\n",
       "    Zon_KE_eddy  (tau, zl, yh, xq) float64 dask.array<chunksize=(1, 2, 800, 399), meta=np.ndarray>\n",
       "    Mer_KE_eddy  (tau, zl, yq, xh) float64 dask.array<chunksize=(1, 2, 799, 400), meta=np.ndarray>\n",
       "    taux_bot     (tau, yh, xq) float32 dask.array<chunksize=(1, 800, 400), meta=np.ndarray>\n",
       "    tauy_bot     (tau, yq, xh) float32 dask.array<chunksize=(1, 800, 400), meta=np.ndarray>\n",
       "    taux         (tau, yh, xh) float64 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
       "    depth        (yh, xh) float64 4e+03 4e+03 4e+03 4e+03 ... 4e+03 4e+03 4e+03"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FW.to_netcdf(save_pth + case + '_FW_mean.nc')\n",
    "VW.to_netcdf(save_pth + case + '_VW_mean.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_FW.close()\n",
    "ds_VW.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using snapshot fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prog.nc files for all runs (for 40-50 years simulation time. The simulations are in equilibrium state)\n",
    "tau = ['Tau_001', 'Tau_003', 'Tau_005', 'Tau_01', 'Tau_03', 'Tau_05', 'Tau_1']\n",
    "ppdir = \"/work/Hemant.Khatri/MOM6_idealised/SWM_Two_Layer/Barotropic_Baroclinic/\"\n",
    "\n",
    "dataset_FW = []\n",
    "dataset_VW = []\n",
    "\n",
    "case = 'betaplane'  # valid options are betaplane, fplane\n",
    "\n",
    "t_st = 1 # starting time for averaging (use t_st = 180 for fplane as these runs used restart files from betaplane)\n",
    "\n",
    "for i in range(0, len(tau)):\n",
    "    if (case == 'betaplane'):\n",
    "        file1 = ppdir+\"2L_Fix_Width/\"+tau[i]+\"/prog_50.nc\"\n",
    "        file2 = ppdir+\"2L_Var_Width/\"+tau[i]+\"/prog_50.nc\"\n",
    "    elif (case == 'fplane'):\n",
    "        file1 = ppdir+\"2L_fplane_Fix_Width/\"+tau[i]+\"/prog_50.nc\"\n",
    "        file2 = ppdir+\"2L_fplane_Var_Width/\"+tau[i]+\"/prog_50.nc\"\n",
    "    else:\n",
    "        sys.exit(\"case selection no valid\")\n",
    "        \n",
    "    d = xr.open_dataset(file1, chunks = {'Time': 1}, decode_times=False) # chunks required\n",
    "    dataset_FW.append(d)\n",
    "    d = xr.open_dataset(file2, chunks = {'Time': 1}, decode_times=False) # for parallel compu.\n",
    "    dataset_VW.append(d)\n",
    "    \n",
    "ds_FW = xr.concat(dataset_FW, dim='tau')\n",
    "ds_VW = xr.concat(dataset_VW, dim='tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select time range\n",
    "ds_FW = ds_FW.isel(Time = slice(t_st, len(ds_FW['Time']) -1))\n",
    "ds_VW = ds_VW.isel(Time = slice(t_st, len(ds_VW['Time']) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (Time: 363, tau: 7, xh: 400, xq: 400, yh: 800, yq: 800, zl: 2)\n",
       "Coordinates:\n",
       "  * yh       (yh) float64 -1.598e+03 -1.594e+03 ... 1.594e+03 1.598e+03\n",
       "  * yq       (yq) float64 -1.596e+03 -1.592e+03 -1.588e+03 ... 1.596e+03 1.6e+03\n",
       "  * Time     (Time) float64 1.462e+04 1.463e+04 ... 1.823e+04 1.824e+04\n",
       "  * xh       (xh) float64 2.0 6.0 10.0 14.0 ... 1.59e+03 1.594e+03 1.598e+03\n",
       "  * xq       (xq) float64 4.0 8.0 12.0 16.0 ... 1.592e+03 1.596e+03 1.6e+03\n",
       "  * zl       (zl) float64 1.027e+03 1.028e+03\n",
       "Dimensions without coordinates: tau\n",
       "Data variables:\n",
       "    u        (tau, Time, zl, yh, xq) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    v        (tau, Time, zl, yq, xh) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    h        (tau, Time, zl, yh, xh) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    uh       (tau, Time, zl, yh, xq) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "    vh       (tau, Time, zl, yq, xh) float32 dask.array<chunksize=(1, 1, 2, 800, 400), meta=np.ndarray>\n",
       "Attributes:\n",
       "    filename:   prog.nc\n",
       "    title:      Channel\n",
       "    grid_type:  regular\n",
       "    grid_tile:  N/A"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_FW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large \\text{Computations}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid create for computations\n",
    "grid_FW = Grid(ds_FW, coords={'X': {'center': 'xh', 'right': 'xq'},\n",
    "                        'Y': {'center': 'yh', 'right': 'yq'}}, periodic=['X']);\n",
    "\n",
    "grid_VW = Grid(ds_VW, coords={'X': {'center': 'xh', 'right': 'xq'},\n",
    "                        'Y': {'center': 'yh', 'right': 'yq'}}, periodic=['X']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean_PV(ds, grid, case):\n",
    "    \n",
    "    f0, beta = (-1.0 * 1e-4, 1.5 * 1e-11)\n",
    "    dx, dy = (4000., 4000.)\n",
    "    \n",
    "    if (case == 'betaplane'):\n",
    "        Coriolis = f0 + beta * ds['yq'] * 1000. # convert to m.\n",
    "    elif (case == 'fplane'):\n",
    "        Coriolis = f0\n",
    "    else:\n",
    "        sys.exit(\"case selection no valid\")\n",
    "    \n",
    "    # compute vorticity\n",
    "    zeta = ( - grid.diff(ds['u'], 'Y', boundary='fill') / dy + grid.diff(ds['v'], 'X', boundary='fill') / dx )\n",
    "    \n",
    "    h_q = grid.interp(grid.interp(ds['h'], 'X', boundary='fill'), 'Y', boundary='fill')\n",
    "    \n",
    "    # compute pv\n",
    "    pv_f = (Coriolis / h_q).mean('Time')\n",
    "    pv_zeta = (zeta / h_q).mean('Time')\n",
    "    \n",
    "    ds1 = xr.Dataset()\n",
    "    \n",
    "    ds1['PV_f'] = pv_f\n",
    "    ds1['PV_zeta'] = pv_zeta\n",
    "    \n",
    "    ds1 = ds1.transpose('tau','zl', 'yq', 'xq')\n",
    "    \n",
    "    return ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (tau: 7, xq: 400, yq: 800, zl: 2)\n",
      "Coordinates:\n",
      "  * yq       (yq) float64 -1.596e+03 -1.592e+03 -1.588e+03 ... 1.596e+03 1.6e+03\n",
      "  * zl       (zl) float64 1.027e+03 1.028e+03\n",
      "  * xq       (xq) float64 4.0 8.0 12.0 16.0 ... 1.592e+03 1.596e+03 1.6e+03\n",
      "Dimensions without coordinates: tau\n",
      "Data variables:\n",
      "    PV_f     (tau, zl, yq, xq) float64 dask.array<chunksize=(1, 2, 799, 399), meta=np.ndarray>\n",
      "    PV_zeta  (tau, zl, yq, xq) float32 dask.array<chunksize=(1, 2, 799, 399), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "save_pth = \"/work/Hemant.Khatri/MOM6_idealised/SWM_Two_Layer/Barotropic_Baroclinic/Postprocessing/\"\n",
    "FW = Mean_PV(ds_FW, grid_FW, case)\n",
    "VW = Mean_PV(ds_VW, grid_VW, case)\n",
    "\n",
    "print(FW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FW.to_netcdf(save_pth + case + '_FW_mean_PV.nc')\n",
    "VW.to_netcdf(save_pth + case + '_VW_mean_PV.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_FW.close()\n",
    "ds_VW.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
